{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、基于PaddleNLP预训练ERNIE模型优化中文地址要素解析\n",
    "\n",
    "[“英特尔创新大师杯”深度学习挑战赛 赛道2：CCKS2021中文NLP地址要素解析-天池大赛-阿里云天池](https://tianchi.aliyun.com/competition/entrance/531900/information)\n",
    "\n",
    "\n",
    "## 1.赛题描述\n",
    "中文地址要素解析任务的目标即将一条地址分解为上述几个部分的详细标签，如：\n",
    "\n",
    "输入：浙江省杭州市余杭区五常街道文一西路969号淘宝城5号楼，放前台\n",
    "输出：Province=浙江省 city=杭州市 district=余杭区 town=五常街道 road=文一西路road_number=969号 poi=淘宝城 house_number=5号楼 other=，放前台\n",
    "\n",
    "## 2.数据说明\n",
    "标注数据集由训练集、验证集和测试集组成，整体标注数据大约2万条左右。地址数据通过抓取公开的地址信息（如黄页网站等）获得， 均通过众包标注生成，详细标注规范将会在数据发布时一并给出。\n",
    "\n",
    "\n",
    "## 3.命名实体识别介绍\n",
    "命名实体识别是NLP中一项非常基础的任务，是信息提取、问答系统、句法分析、机器翻译等众多NLP任务的重要基础工具。命名实体识别的准确度，决定了下游任务的效果，是NLP中的一个基础问题。在NER任务提供了两种解决方案，一类LSTM/GRU + CRF，RNN类的模型来抽取底层文本的信息，而CRF(条件随机场)模型来学习底层Token之间的联系；另外一类是通过预训练模型，例如ERNIE，BERT模型，直接来预测Token的标签信息。\n",
    "\n",
    "本项目将演示，如何使用PaddleNLP语义预训练模型ERNIE完成从快递单中抽取姓名、电话、省、市、区、详细地址等内容，形成结构化信息。辅助物流行业从业者进行有效信息的提取，从而降低客户填单的成本，完成比赛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、RNN命名实体识别概念\n",
    "在2017年之前，工业界和学术界对NLP文本处理依赖于序列模型[Recurrent Neural Network (RNN)](https://baike.baidu.com/item/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/23199490?fromtitle=RNN&fromid=5707183&fr=aladdin).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\" width=\"40%\" height=\"30%\"> <br />\n",
    "</p><br><center>图1：RNN示意图</center></br>\n",
    "\n",
    "[基于BiGRU+CRF的快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)项目介绍了如何使用序列模型完成快递单信息抽取任务。\n",
    "<br>\n",
    "\n",
    "近年来随着深度学习的发展，模型参数的数量飞速增长。为了训练这些参数，需要更大的数据集来避免过拟合。然而，对于大部分NLP任务来说，构建大规模的标注数据集非常困难（成本过高），特别是对于句法和语义相关的任务。相比之下，大规模的未标注语料库的构建则相对容易。为了利用这些数据，我们可以先从其中学习到一个好的表示，再将这些表示应用到其他任务中。最近的研究表明，基于大规模未标注语料库的预训练模型（Pretrained Models, PTM) 在NLP任务上取得了很好的表现。\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的发展，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p><br><center>图2：预训练模型一览，图片来源于：https://github.com/thunlp/PLMpapers</center></br>\n",
    "                                                                                                                             \n",
    "本示例展示了以ERNIE([Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223))代表的预训练模型如何Finetune完成序列标注任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.PaddleNLP环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from utils import convert_example, evaluate, predict, load_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.数据整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!unzip 'data/data94613/“英特尔创新大师杯”深度学习挑战赛 赛道2：CCKS2021中文NLP地址要素解析.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mv 'б░╙в╠╪╢√┤┤╨┬┤є╩ж▒нб▒╔ю╢╚╤з╧░╠Ї╒╜╚№ ╚№╡└2г║CCKS2021╓╨╬─NLP╡╪╓╖╥к╦╪╜т╬Ў' dataset\r\n",
    "!mv 'dataset/╓╨╬─╡╪╓╖╥к╦╪╜т╬Ў▒ъ╫в╣ц╖╢.pdf' dastaset/中文地址要素解析标注规范.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.数据查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浙 B-prov\r\n",
      "江 E-prov\r\n",
      "杭 B-city\r\n",
      "州 I-city\r\n",
      "市 E-city\r\n",
      "江 B-district\r\n",
      "干 I-district\r\n",
      "区 E-district\r\n",
      "九 B-town\r\n",
      "堡 I-town\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 dataset/train.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杭 B-city\r\n",
      "州 E-city\r\n",
      "五 B-poi\r\n",
      "洲 I-poi\r\n",
      "国 I-poi\r\n",
      "际 E-poi\r\n",
      "\r\n",
      "浙 B-prov\r\n",
      "江 I-prov\r\n",
      "省 E-prov\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10  dataset/dev.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\u0001朝阳区小关北里000-0号\r\n",
      "2\u0001朝阳区惠新东街00号\r\n",
      "3\u0001朝阳区南磨房路与西大望路交口东南角\r\n",
      "4\u0001朝阳区潘家园南里00号\r\n",
      "5\u0001朝阳区向军南里二巷0号附近\r\n",
      "6\u0001朝阳区多处营业网点\r\n",
      "7\u0001朝阳区多处营业网点\r\n",
      "8\u0001朝阳区多处营业网点\r\n",
      "9\u0001朝阳区北三环中路00号商房大厦0楼\r\n",
      "10\u0001朝阳区孙河乡康营家园00区北侧底商\r\n"
     ]
    }
   ],
   "source": [
    "!head dataset/final_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.数据格式调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "\r\n",
    "def format_data(source_filename, target_filename):\r\n",
    "    datalist=[]\r\n",
    "    with open(source_filename, 'r', encoding='utf-8') as f:\r\n",
    "        lines=f.readlines()\r\n",
    "    words=''\r\n",
    "    labels=''\r\n",
    "    flag=0\r\n",
    "    for line in lines:  \r\n",
    "        if line == '\\n':\r\n",
    "            item=words+'\\t'+labels+'\\n'\r\n",
    "            # print(item)\r\n",
    "            datalist.append(item)\r\n",
    "            words=''\r\n",
    "            labels=''\r\n",
    "            flag=0\r\n",
    "            continue\r\n",
    "        word, label = line.strip('\\n').split(' ')\r\n",
    "        if flag==1:\r\n",
    "            words=words+'\\002'+word\r\n",
    "            labels=labels+'\\002'+label\r\n",
    "        else:\r\n",
    "            words=words+word\r\n",
    "            labels=labels+label\r\n",
    "            flag=1\r\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\r\n",
    "        lines=f.writelines(datalist)\r\n",
    "    print(f'{source_filename}文件格式转换完毕，保存为{target_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/dev.conll文件格式转换完毕，保存为dataset/dev.txt\n",
      "dataset/train.conll文件格式转换完毕，保存为dataset/train.txt\n"
     ]
    }
   ],
   "source": [
    "format_data('dataset/dev.conll', 'dataset/dev.txt')\r\n",
    "format_data(r'dataset/train.conll', r'dataset/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杭\u0002州\u0002五\u0002洲\u0002国\u0002际\tB-city\u0002E-city\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\r\n",
      "浙\u0002江\u0002省\u0002杭\u0002州\u0002市\u0002余\u0002杭\u0002乔\u0002司\u0002街\u0002道\u0002博\u0002卡\u0002路\u00020\u0002号\u0002博\u0002卡\u0002制\u0002衣\tB-prov\u0002I-prov\u0002E-prov\u0002B-city\u0002I-city\u0002E-city\u0002B-district\u0002E-district\u0002B-town\u0002I-town\u0002I-town\u0002E-town\u0002B-road\u0002I-road\u0002E-road\u0002B-roadno\u0002E-roadno\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\r\n",
      "浙\u0002江\u0002诸\u0002暨\u0002市\u0002暨\u0002阳\u0002八\u0002一\u0002新\u0002村\u00020\u00020\u0002幢\tB-prov\u0002E-prov\u0002B-district\u0002I-district\u0002E-district\u0002B-town\u0002E-town\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\u0002B-houseno\u0002I-houseno\u0002E-houseno\r\n",
      "杭\u0002州\u0002市\u0002武\u0002林\u0002广\u0002场\u0002杭\u0002州\u0002大\u0002厦\u0002商\u0002城\u0002A\u0002座\u0002九\u0002层\tB-city\u0002I-city\u0002E-city\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\u0002B-subpoi\u0002I-subpoi\u0002I-subpoi\u0002E-subpoi\u0002B-subpoi\u0002E-subpoi\u0002B-houseno\u0002E-houseno\u0002B-floorno\u0002E-floorno\r\n",
      "浙\u0002江\u0002省\u0002杭\u0002州\u0002市\u0002拱\u0002墅\u0002区\u0002登\u0002云\u0002路\u00020\u00020\u00020\u00020\u0002号\u0002时\u0002代\u0002电\u0002子\u0002市\u0002场\tB-prov\u0002I-prov\u0002E-prov\u0002B-city\u0002I-city\u0002E-city\u0002B-district\u0002I-district\u0002E-district\u0002B-road\u0002I-road\u0002E-road\u0002B-roadno\u0002I-roadno\u0002I-roadno\u0002I-roadno\u0002E-roadno\u0002B-poi\u0002I-poi\u0002I-poi\u0002I-poi\u0002I-poi\u0002E-poi\r\n",
      "浙\u0002江\u0002省\u0002宁\u0002波\u0002市\u0002慈\u0002溪\u0002市\u0002宗\u0002汉\u0002街\u0002道\u0002联\u0002丰\u0002公\u0002寓\u00020\u00020\u0002栋\tB-prov\u0002I-prov\u0002E-prov\u0002B-city\u0002I-city\u0002E-city\u0002B-district\u0002I-district\u0002E-district\u0002B-town\u0002I-town\u0002I-town\u0002E-town\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\u0002B-houseno\u0002I-houseno\u0002E-houseno\r\n",
      "浙\u0002江\u0002省\u0002温\u0002州\u0002市\u0002鹿\u0002城\u0002区\u0002劳\u0002务\u0002市\u0002场\u0002跨\u0002境\u0002电\u0002商\u0002园\u00020\u00020\u0002楼\u0002艺\u0002网\u0002科\u0002技\u0002有\u0002限\u0002公\u0002司\tB-prov\u0002I-prov\u0002E-prov\u0002B-city\u0002I-city\u0002E-city\u0002B-district\u0002I-district\u0002E-district\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\u0002B-devzone\u0002I-devzone\u0002I-devzone\u0002I-devzone\u0002E-devzone\u0002B-floorno\u0002I-floorno\u0002E-floorno\u0002B-subpoi\u0002I-subpoi\u0002I-subpoi\u0002I-subpoi\u0002I-subpoi\u0002I-subpoi\u0002I-subpoi\u0002E-subpoi\r\n",
      "康\u0002中\u0002路\u00020\u00020\u0002号\u0002康\u0002城\u0002工\u0002业\u0002园\u00020\u00020\u0002幢\u00020\u0002楼\tB-road\u0002I-road\u0002E-road\u0002B-roadno\u0002I-roadno\u0002E-roadno\u0002B-devzone\u0002I-devzone\u0002I-devzone\u0002I-devzone\u0002E-devzone\u0002B-houseno\u0002I-houseno\u0002E-houseno\u0002B-floorno\u0002E-floorno\r\n",
      "金\u0002华\u0002永\u0002康\u0002市\u0002城\u0002西\u0002工\u0002业\u0002区\u0002蓝\u0002天\u0002路\u0002坊\u0002培\u0002电\u0002脑\tB-city\u0002E-city\u0002B-district\u0002I-district\u0002E-district\u0002B-devzone\u0002I-devzone\u0002I-devzone\u0002I-devzone\u0002E-devzone\u0002B-road\u0002I-road\u0002E-road\u0002B-poi\u0002I-poi\u0002I-poi\u0002E-poi\r\n",
      "宜\u0002山\u0002人\u0002民\u0002路\u00020\u00020\u00020\u00020\u0002号\u0002后\u0002栋\u0002纸\u0002巾\u0002厂\tB-town\u0002E-town\u0002B-road\u0002I-road\u0002E-road\u0002B-roadno\u0002I-roadno\u0002I-roadno\u0002I-roadno\u0002E-roadno\u0002B-houseno\u0002E-houseno\u0002B-poi\u0002I-poi\u0002E-poi\r\n"
     ]
    }
   ],
   "source": [
    "!head dataset/dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.加载自定义数据集\n",
    "\n",
    "推荐使用MapDataset()自定义数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            next(fp)  # Skip header\n",
    "            for line in fp.readlines():\n",
    "                words, labels = line.strip('\\n').split('\\t')\n",
    "                words = words.split('\\002')\n",
    "                labels = labels.split('\\002')\n",
    "                yield words, labels\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataset, tokenizer and dataloader.\r\n",
    "train_ds, dev_ds = load_dataset(datafiles=(\r\n",
    "        './dataset/train.txt', './dataset/dev.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['浙', '江', '省', '温', '州', '市', '平', '阳', '县', '海', '西', '镇', '宋', '埠', '公', '园', '南', '路', '0', '0', '0', '0', '号'], ['B-prov', 'I-prov', 'E-prov', 'B-city', 'I-city', 'E-city', 'B-district', 'I-district', 'E-district', 'B-town', 'I-town', 'E-town', 'B-poi', 'I-poi', 'I-poi', 'E-poi', 'B-road', 'E-road', 'B-roadno', 'I-roadno', 'I-roadno', 'I-roadno', 'E-roadno'])\n",
      "(['浙', '江', '省', '余', '姚', '市', '模', '具', '城', '金', '型', '路', '0', '0', '0', '号', '_', '样', '样', '红', '0', 'A', '打', '印'], ['B-prov', 'I-prov', 'E-prov', 'B-district', 'I-district', 'E-district', 'B-poi', 'I-poi', 'E-poi', 'B-road', 'I-road', 'E-road', 'B-roadno', 'I-roadno', 'I-roadno', 'E-roadno', 'O', 'B-subpoi', 'I-subpoi', 'I-subpoi', 'I-subpoi', 'I-subpoi', 'I-subpoi', 'E-subpoi'])\n",
      "(['浙', '江', '省', '杭', '州', '市', '江', '干', '区', '白', '杨', '街', '道', '下', '沙', '开', '发', '区', '世', '茂', '江', '滨', '花', '园', '峻', '景', '湾', '0', '0', '幢'], ['B-prov', 'I-prov', 'E-prov', 'B-city', 'I-city', 'E-city', 'B-district', 'I-district', 'E-district', 'B-town', 'I-town', 'I-town', 'E-town', 'B-devzone', 'I-devzone', 'I-devzone', 'I-devzone', 'E-devzone', 'B-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'E-poi', 'B-subpoi', 'I-subpoi', 'E-subpoi', 'B-houseno', 'I-houseno', 'E-houseno'])\n",
      "(['秋', '菱', '路', '浙', '江', '兰', '溪', '金', '立', '达', '框', '业', '有', '限', '公', '司'], ['B-road', 'I-road', 'E-road', 'B-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'E-poi'])\n",
      "(['南', '湖', '区', '中', '环', '南', '路', '和', '花', '园', '路', '交', '叉', '口', '嘉', '兴', '市', '城', '乡', '规', '划', '建', '设', '管', '理', '委', '员', '会'], ['B-district', 'I-district', 'E-district', 'B-road', 'I-road', 'I-road', 'E-road', 'O', 'B-road', 'I-road', 'E-road', 'B-intersection', 'I-intersection', 'E-intersection', 'B-city', 'I-city', 'E-city', 'B-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'I-poi', 'E-poi'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6 label标签表构建\n",
    "每条数据包含一句文本和这个文本中每个汉字以及数字对应的label标签，具体对应关系见 **中文地址要素解析标注规范.pdf**\n",
    "\n",
    "之后，还需要对输入句子进行数据处理，如切词，映射词表id等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gernate_dic(source_filename1, source_filename2, target_filename):\r\n",
    "    data_list=[]\r\n",
    "\r\n",
    "    with open(source_filename1, 'r', encoding='utf-8') as f:\r\n",
    "        lines=f.readlines()\r\n",
    "\r\n",
    "    for line in lines:\r\n",
    "        if line != '\\n':\r\n",
    "            dic=line.strip('\\n').split(' ')[-1]\r\n",
    "            if dic+'\\n' not in data_list:\r\n",
    "                data_list.append(dic+'\\n')\r\n",
    "    \r\n",
    "    with open(source_filename2, 'r', encoding='utf-8') as f:\r\n",
    "        lines=f.readlines()\r\n",
    "\r\n",
    "    for line in lines:\r\n",
    "        if line != '\\n':\r\n",
    "            dic=line.strip('\\n').split(' ')[-1]\r\n",
    "            if dic+'\\n' not in data_list:\r\n",
    "                data_list.append(dic+'\\n')\r\n",
    "\r\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\r\n",
    "        lines=f.writelines(data_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从dev文件生成dic\r\n",
    "gernate_dic('dataset/train.conll', 'dataset/dev.conll', 'dataset/mytag.dic')\r\n",
    "# gernate_dic('dataset/dev.conll', 'dataset/mytag_dev.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-prov\r\n",
      "E-prov\r\n",
      "B-city\r\n",
      "I-city\r\n",
      "E-city\r\n",
      "B-district\r\n",
      "I-district\r\n",
      "E-district\r\n",
      "B-town\r\n",
      "I-town\r\n",
      "E-town\r\n",
      "B-community\r\n",
      "I-community\r\n",
      "E-community\r\n",
      "B-poi\r\n",
      "E-poi\r\n",
      "I-prov\r\n",
      "I-poi\r\n",
      "B-road\r\n",
      "E-road\r\n",
      "B-roadno\r\n",
      "I-roadno\r\n",
      "E-roadno\r\n",
      "I-road\r\n",
      "O\r\n",
      "B-subpoi\r\n",
      "I-subpoi\r\n",
      "E-subpoi\r\n",
      "B-devzone\r\n",
      "I-devzone\r\n",
      "E-devzone\r\n",
      "B-houseno\r\n",
      "I-houseno\r\n",
      "E-houseno\r\n",
      "B-intersection\r\n",
      "I-intersection\r\n",
      "E-intersection\r\n",
      "B-assist\r\n",
      "I-assist\r\n",
      "E-assist\r\n",
      "B-cellno\r\n",
      "I-cellno\r\n",
      "E-cellno\r\n",
      "B-floorno\r\n",
      "E-floorno\r\n",
      "S-assist\r\n",
      "I-floorno\r\n",
      "B-distance\r\n",
      "I-distance\r\n",
      "E-distance\r\n",
      "B-village_group\r\n",
      "E-village_group\r\n",
      "I-village_group\r\n",
      "S-poi\r\n",
      "S-intersection\r\n",
      "S-district\r\n",
      "S-community\r\n"
     ]
    }
   ],
   "source": [
    "# 查看生成的dic文件\r\n",
    "!cat dataset/mytag.dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.数据处理\n",
    "\n",
    "预训练模型ERNIE对中文数据的处理是以字为单位。PaddleNLP对于各种预训练模型已经内置了相应的tokenizer。指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer作用为将原始输入文本转化成模型model可以接受的输入数据形式。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "<br><center>图3：ERNIE模型示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-28 13:26:34,755] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt\n",
      "100%|██████████| 90/90 [00:00<00:00, 4654.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 1382, 409, 244, 565, 404, 99, 157, 507, 308, 233, 213, 484, 945, 3074, 53, 509, 219, 216, 540, 540, 540, 540, 500, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 25, [24, 0, 16, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 17, 17, 15, 18, 19, 20, 21, 21, 21, 22, 24])\n"
     ]
    }
   ],
   "source": [
    "label_vocab = load_dict('./dataset/mytag.dic')\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\n",
    "\n",
    "train_ds.map(trans_func)\n",
    "dev_ds.map(trans_func)\n",
    "print (train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据读入\n",
    "\n",
    "使用`paddle.io.DataLoader`接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = -1\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=300,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=300,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、PaddleNLP一键加载预训练模型\n",
    "\n",
    "## 1.加载预训练模型\n",
    "\n",
    "快递单信息抽取本质是一个序列标注任务，PaddleNLP对于各种预训练模型已经内置了对于下游任务文本分类Fine-tune网络。以下教程以ERNIE为预训练模型完成序列标注任务。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification()`一行代码即可加载预训练模型ERNIE用于序列标注任务的fine-tune网络。其在ERNIE模型后拼接上一个全连接网络进行分类。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification.from_pretrained()`方法只需指定想要使用的模型名称和文本分类的类别数即可完成定义模型网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-28 13:26:34,864] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
      "[2021-06-28 13:26:34,866] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams\n",
      "100%|██████████| 392507/392507 [00:08<00:00, 48559.94it/s]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "# Define the model netword and its loss\n",
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PaddleNLP不仅支持ERNIE预训练模型，还支持BERT、RoBERTa、Electra等预训练模型。\n",
    "下表汇总了目前PaddleNLP支持的各类预训练模型。您可以使用PaddleNLP提供的模型，完成文本分类、序列标注、问答等任务。同时我们提供了众多预训练模型的参数权重供用户使用，其中包含了二十多种中文语言模型的预训练权重。中文的预训练模型有`bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, gpt2-base-cn, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small, chinese-xlnet-base, chinese-xlnet-mid, chinese-xlnet-large, unified_transformer-12L-cn, unified_transformer-12L-cn-luge`等。\n",
    "\n",
    "更多预训练模型参考：[PaddleNLP Transformer API](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)。\n",
    "\n",
    "更多预训练模型fine-tune下游任务使用方法，请参考：[examples](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.设置Fine-Tune优化策略，模型配置\n",
    "适用于ERNIE/BERT这类Transformer模型的迁移优化学习率策略为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p><br><center>图4：动态学习率示意图</center></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、模型训练与评估\n",
    "\n",
    "## 1.训练模型\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。将前向计算结果传给评价方法，计算评价指标。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序将会评估一次，评估当前模型训练的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "for epoch in range(50):\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        step += 1\n",
    "        print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\n",
    "    evaluate(model, metric, dev_loader)\n",
    "\n",
    "    paddle.save(model.state_dict(),\n",
    "                './checkpoint/model_%d.pdparams' % step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "epoch:49 - step:1832 - loss: 0.057792\n",
    "epoch:49 - step:1833 - loss: 0.053191\n",
    "epoch:49 - step:1834 - loss: 0.051053\n",
    "epoch:49 - step:1835 - loss: 0.054221\n",
    "epoch:49 - step:1836 - loss: 0.036712\n",
    "epoch:49 - step:1837 - loss: 0.038394\n",
    "epoch:49 - step:1838 - loss: 0.045484\n",
    "epoch:49 - step:1839 - loss: 0.068006\n",
    "epoch:49 - step:1840 - loss: 0.039057\n",
    "epoch:49 - step:1841 - loss: 0.049253\n",
    "epoch:49 - step:1842 - loss: 0.049330\n",
    "epoch:49 - step:1843 - loss: 0.051696\n",
    "epoch:49 - step:1844 - loss: 0.042183\n",
    "epoch:49 - step:1845 - loss: 0.041376\n",
    "epoch:49 - step:1846 - loss: 0.040038\n",
    "epoch:49 - step:1847 - loss: 0.046694\n",
    "epoch:49 - step:1848 - loss: 0.043038\n",
    "epoch:49 - step:1849 - loss: 0.046348\n",
    "epoch:49 - step:1850 - loss: 0.007658\n",
    "eval precision: 0.997797 - recall: 0.998420 - f1: 0.998109\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir ernie_result\r\n",
    "model.save_pretrained('./ernie_result')\r\n",
    "tokenizer.save_pretrained('./ernie_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、预测\n",
    "\n",
    "训练保存好的训练，即可用于预测。如以下示例代码自定义预测数据，调用`predict()`函数即可一键预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.io import DataLoader\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from utils import convert_example, evaluate, predict, load_dict\r\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\u0001朝阳区小关北里000-0号\r\n",
      "2\u0001朝阳区惠新东街00号\r\n",
      "3\u0001朝阳区南磨房路与西大望路交口东南角\r\n",
      "4\u0001朝阳区潘家园南里00号\r\n",
      "5\u0001朝阳区向军南里二巷0号附近\r\n",
      "6\u0001朝阳区多处营业网点\r\n",
      "7\u0001朝阳区多处营业网点\r\n",
      "8\u0001朝阳区多处营业网点\r\n",
      "9\u0001朝阳区北三环中路00号商房大厦0楼\r\n",
      "10\u0001朝阳区孙河乡康营家园00区北侧底商\r\n",
      "11\u0001朝阳区将台乡雍家村\r\n",
      "12\u0001朝阳区安家楼村路\r\n",
      "13\u0001朝阳区郎辛庄北路\r\n",
      "14\u0001朝阳区酒仙桥路0号院0号楼一层\r\n",
      "15\u0001朝阳区十里堡北里南区0号楼0楼\r\n",
      "16\u0001朝阳区双桥医院\r\n",
      "17\u0001朝阳区五里桥一街甲0号中弘北京像素北区0号楼0单元0000号\r\n",
      "18\u0001朝阳区傲城融富中心A座0000\r\n",
      "19\u0001朝阳区西坝河西里00号英特公寓A0座0000室\r\n",
      "20\u0001朝阳区姚家园路00号院\r\n"
     ]
    }
   ],
   "source": [
    "!head -n20 dataset/final_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.定义test数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\r\n",
    "    def read(data_path):\r\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\r\n",
    "            # next(fp)  # 没有header，不用Skip header\r\n",
    "            for line in fp.readlines():\r\n",
    "                ids, words = line.strip('\\n').split('\\001')\r\n",
    "                words=[ch for ch in words]\r\n",
    "                # 要预测的数据集没有label，伪造个O，不知道可以不 ，应该后面预测不会用label\r\n",
    "                labels=['O' for x in range(0,len(words))]\r\n",
    "\r\n",
    "                yield words, labels\r\n",
    "                # yield words\r\n",
    "\r\n",
    "    if isinstance(datafiles, str):\r\n",
    "        return MapDataset(list(read(datafiles)))\r\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\r\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataset, tokenizer and dataloader.\r\n",
    "test_ds = load_dataset(datafiles=('./dataset/final_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['朝', '阳', '区', '小', '关', '北', '里', '0', '0', '0', '-', '0', '号'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n",
      "(['朝', '阳', '区', '惠', '新', '东', '街', '0', '0', '号'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n",
      "(['朝', '阳', '区', '南', '磨', '房', '路', '与', '西', '大', '望', '路', '交', '口', '东', '南', '角'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\r\n",
    "    print(test_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.加载训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_vocab = load_dict('./dataset/mytag.dic')\r\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\r\n",
    "\r\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\r\n",
    "test_ds.map(trans_func)\r\n",
    "print (test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = 1\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack(),  # seq_len\r\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\r\n",
    "): fn(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = paddle.io.DataLoader(\r\n",
    "    dataset=test_ds,\r\n",
    "    batch_size=30,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_predict(model, data_loader, ds, label_vocab):\r\n",
    "    pred_list = []\r\n",
    "    len_list = []\r\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\r\n",
    "        logits = model(input_ids, seg_ids)\r\n",
    "        # print(len(logits[0]))\r\n",
    "        pred = paddle.argmax(logits, axis=-1)\r\n",
    "        pred_list.append(pred.numpy())\r\n",
    "        len_list.append(lens.numpy())\r\n",
    "    preds ,tags= parse_decodes(ds, pred_list, len_list, label_vocab)\r\n",
    "    return preds, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "# Define the model netword and its loss\r\n",
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dict = paddle.load('ernie_result/model_state.pdparams')\r\n",
    "model.set_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.预测并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\r\n",
    "preds, tags = my_predict(model, test_loader, test_ds, label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: ernie_results.txt, some examples are shown below: \n"
     ]
    }
   ],
   "source": [
    "file_path = \"ernie_results.txt\"\r\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\r\n",
    "    fout.write(\"\\n\".join(preds))\r\n",
    "# Print some examples\r\n",
    "print(\r\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\r\n",
    "    % file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " B-district I-district E-district B-road I-road I-road E-road B-roadno I-roadno I-roadno I-roadno I-roadno E-roadno\n",
      " B-district I-district E-district B-road I-road I-road E-road B-roadno I-roadno E-roadno\n",
      " B-district I-district E-district B-road I-road I-road E-road O B-road I-road I-road E-road B-intersection E-intersection B-assist I-assist E-assist\n",
      " B-district I-district E-district B-poi I-poi E-poi B-road E-road B-houseno I-houseno E-houseno\n",
      " B-district I-district E-district B-road I-road I-road E-road B-road E-road B-roadno E-roadno B-assist E-assist\n",
      " B-district I-district E-district B-poi I-poi I-poi I-poi I-poi E-poi\n",
      " B-district I-district E-district B-poi I-poi B-poi I-poi I-poi E-poi\n",
      " B-district I-district E-district B-poi I-poi I-poi I-poi I-poi E-poi\n",
      " B-district I-district E-district B-road I-road I-road I-road E-road B-roadno I-roadno E-roadno B-poi I-poi I-poi E-poi B-floorno E-floorno\n",
      " B-district I-district E-district B-town I-town E-town B-poi I-poi I-poi E-poi B-subpoi I-subpoi E-subpoi B-assist E-assist O E-subpoi\n",
      " B-district I-district E-district B-town I-town E-town B-community I-community E-community\n",
      " B-district I-district E-district B-community I-community I-community E-community O\n",
      " B-district I-district E-district B-road I-road I-road I-road E-road\n",
      " B-district I-district E-district B-road I-road I-road E-road B-poi I-poi E-poi B-houseno I-houseno E-houseno O O\n",
      " B-district I-district E-district B-poi I-poi E-poi I-poi E-poi B-subpoi E-poi B-houseno I-houseno E-houseno B-floorno E-floorno\n",
      " B-district I-district E-district B-poi I-poi I-poi E-poi\n",
      " B-district I-district E-district B-road I-road I-road I-road E-road B-roadno I-roadno E-roadno B-poi I-poi I-poi I-poi I-poi I-poi I-poi E-poi B-houseno I-houseno E-houseno B-cellno I-cellno E-cellno O I-houseno I-houseno I-houseno E-houseno\n",
      " B-district I-district E-district B-poi I-poi I-poi I-poi I-poi E-poi B-houseno E-houseno O O O O\n",
      " B-district I-district E-district B-road I-road I-road B-road E-road B-roadno I-roadno E-roadno B-poi I-poi I-poi E-poi B-houseno I-houseno E-houseno O O O O E-floorno\n",
      " B-district I-district E-district B-road I-road I-road E-road B-poi I-poi I-poi E-poi\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(preds[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " B-district I-district E-district B-road I-road I-road E-road B-roadno I-roadno I-roadno I-roadno I-roadno E-roadno\r\n",
      " B-district I-district E-district B-road I-road I-road E-road B-roadno I-roadno E-roadno\r\n",
      " B-district I-district E-district B-road I-road I-road E-road O B-road I-road I-road E-road B-intersection E-intersection B-assist I-assist E-assist\r\n",
      " B-district I-district E-district B-poi I-poi E-poi B-road E-road B-houseno I-houseno E-houseno\r\n",
      " B-district I-district E-district B-road I-road I-road E-road B-road E-road B-roadno E-roadno B-assist E-assist\r\n",
      " B-district I-district E-district B-poi I-poi I-poi I-poi I-poi E-poi\r\n",
      " B-district I-district E-district B-poi I-poi B-poi I-poi I-poi E-poi\r\n",
      " B-district I-district E-district B-poi I-poi I-poi I-poi I-poi E-poi\r\n",
      " B-district I-district E-district B-road I-road I-road I-road E-road B-roadno I-roadno E-roadno B-poi I-poi I-poi E-poi B-floorno E-floorno\r\n",
      " B-district I-district E-district B-town I-town E-town B-poi I-poi I-poi E-poi B-subpoi I-subpoi E-subpoi B-assist E-assist O E-subpoi\r\n"
     ]
    }
   ],
   "source": [
    "!head ernie_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\u0001朝阳区小关北里000-0号\r\n",
      "2\u0001朝阳区惠新东街00号\r\n",
      "3\u0001朝阳区南磨房路与西大望路交口东南角\r\n",
      "4\u0001朝阳区潘家园南里00号\r\n",
      "5\u0001朝阳区向军南里二巷0号附近\r\n",
      "6\u0001朝阳区多处营业网点\r\n",
      "7\u0001朝阳区多处营业网点\r\n",
      "8\u0001朝阳区多处营业网点\r\n",
      "9\u0001朝阳区北三环中路00号商房大厦0楼\r\n",
      "10\u0001朝阳区孙河乡康营家园00区北侧底商\r\n"
     ]
    }
   ],
   "source": [
    "!head ./dataset/final_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.转换保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1^ A浙江杭州阿里^AB-prov E-prov B-city E-city B-poi E-poi\n",
      "50000\n",
      "50000\t\t50000\n",
      "**************************************************\n",
      "write result ok!\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "def main():\r\n",
    "    data_list = []\r\n",
    "    with open('ernie_results.txt', encoding='utf-8') as f:\r\n",
    "        data_list = f.readlines()\r\n",
    "    return data_list\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    print('1^ A浙江杭州阿里^AB-prov E-prov B-city E-city B-poi E-poi')\r\n",
    "    sentence_list = main()\r\n",
    "    print(len(sentence_list))\r\n",
    "\r\n",
    "    final_test = []\r\n",
    "    with open('dataset/final_test.txt', encoding='utf-8') as f:\r\n",
    "        final_test = f.readlines()\r\n",
    "    test_data = []\r\n",
    "    print(f'{len(final_test)}\\t\\t{len(sentence_list)}')\r\n",
    "    for i in range(len(final_test)):\r\n",
    "        # test_data.append(final_test[i].strip('\\n') + '\\001' + sentence_list[i] + '\\n')\r\n",
    "        test_data.append(final_test[i].strip('\\n').strip(' ') + '\\001' + sentence_list[i].strip(' '))\r\n",
    "    with open('predict.txt', 'w', encoding='utf-8') as f:\r\n",
    "        f.writelines(test_data)\r\n",
    "    print(50 * '*')\r\n",
    "    print('write result ok!')\r\n",
    "    print(50 * '*')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\u0001朝阳区小关北里000-0号\u0001B-district I-district E-district B-road I-road I-road E-road B-roadno I-roadno I-roadno I-roadno I-roadno E-roadno\r\n",
      "2\u0001朝阳区惠新东街00号\u0001B-district I-district E-district B-road I-road I-road E-road B-roadno I-roadno E-roadno\r\n",
      "3\u0001朝阳区南磨房路与西大望路交口东南角\u0001B-district I-district E-district B-road I-road I-road E-road O B-road I-road I-road E-road B-intersection E-intersection B-assist I-assist E-assist\r\n",
      "4\u0001朝阳区潘家园南里00号\u0001B-district I-district E-district B-poi I-poi E-poi B-road E-road B-houseno I-houseno E-houseno\r\n",
      "5\u0001朝阳区向军南里二巷0号附近\u0001B-district I-district E-district B-road I-road I-road E-road B-road E-road B-roadno E-roadno B-assist E-assist\r\n",
      "6\u0001朝阳区多处营业网点\u0001B-district I-district E-district B-poi I-poi I-poi I-poi I-poi E-poi\r\n",
      "7\u0001朝阳区多处营业网点\u0001B-district I-district E-district B-poi I-poi B-poi I-poi I-poi E-poi\r\n",
      "8\u0001朝阳区多处营业网点\u0001B-district I-district E-district B-poi I-poi I-poi I-poi I-poi E-poi\r\n",
      "9\u0001朝阳区北三环中路00号商房大厦0楼\u0001B-district I-district E-district B-road I-road I-road I-road E-road B-roadno I-roadno E-roadno B-poi I-poi I-poi E-poi B-floorno E-floorno\r\n",
      "10\u0001朝阳区孙河乡康营家园00区北侧底商\u0001B-district I-district E-district B-town I-town E-town B-poi I-poi I-poi E-poi B-subpoi I-subpoi E-subpoi B-assist E-assist O E-subpoi\r\n"
     ]
    }
   ],
   "source": [
    "!head predict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.提交格式检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done ！！\n"
     ]
    }
   ],
   "source": [
    "import linecache\r\n",
    "\r\n",
    "\r\n",
    "def check(submit_path, test_path, max_num=50000):\r\n",
    "    '''\r\n",
    "    :param submit_path: 选手提交的文件名\r\n",
    "    :param test_path: 原始测试数据名\r\n",
    "    :param max_num: 测试数据大小\r\n",
    "    :return:\r\n",
    "    '''\r\n",
    "    N = 0\r\n",
    "    with open(submit_path, 'r', encoding='utf-8') as fin:\r\n",
    "        for line in fin:\r\n",
    "            line = line.strip()\r\n",
    "            if line == '':\r\n",
    "                continue\r\n",
    "            N += 1\r\n",
    "            parts = line.split('\\001')  # id, sent, tags\r\n",
    "            if len(parts) != 3:\r\n",
    "                raise AssertionError(f\"分隔符不正确，写入文件时请用'\\\\001'来分隔ID，句子和预测标签！Error Line:{line.strip()}\")\r\n",
    "            elif len(parts[1]) != len(parts[2].split(' ')):\r\n",
    "                print(line)\r\n",
    "                raise AssertionError(f\"请保证句子长度和标签长度一致，且标签之间用空格分隔！ID:{parts[0]} Sent:{parts[1]}\")\r\n",
    "            elif parts[0] != str(N):\r\n",
    "                raise AssertionError(f\"请保证测试数据的ID合法！ID:{parts[0]} Sent:{parts[1]}\")\r\n",
    "            else:\r\n",
    "                for tag in parts[2].split(' '):\r\n",
    "                    if (tag == 'O' or tag.startswith('S-')\r\n",
    "                        or tag.startswith('B-')\r\n",
    "                        or tag.startswith('I-')\r\n",
    "                        or tag.startswith('E-')) is False:\r\n",
    "                        raise AssertionError(f\"预测结果存在不合法的标签！ID:{parts[0]} Tag:{parts[2]}\")\r\n",
    "\r\n",
    "                test_line = linecache.getline(test_path, int(parts[0]))\r\n",
    "                test_sent = test_line.strip().split('\\001')[1]\r\n",
    "                if test_sent.strip() != parts[1].strip():\r\n",
    "                    raise AssertionError(f\"请不要改变测试数据原文！ID:{parts[0]} Sent:{parts[1]}\")\r\n",
    "\r\n",
    "    if N != max_num:\r\n",
    "        raise AssertionError(f\"请保证测试数据的完整性(共{max_num}条)，不可丢失或增加数据！\")\r\n",
    "\r\n",
    "    print('Well Done ！！')\r\n",
    "\r\n",
    "\r\n",
    "check('predict.txt', 'dataset/final_test.txt')\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 七、终于提交成功了\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/e1cb12b56376403bba2bd3aecef6c10258d9a05817c94326aed836e0ce926685)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
